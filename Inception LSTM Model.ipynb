{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_LSTM_Network.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"qZMySHGcN4U9"},"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import cv2\n","from sklearn.model_selection import train_test_split\n","import time\n","from tqdm import tqdm\n","import pickle\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YsYDAtgIOgZV"},"source":["### Load Data"]},{"cell_type":"markdown","metadata":{"id":"fBDb_tyh1g1a"},"source":["This section loads each video from the directory. The loaded data is then pickled and stored in file named X and Y. If you want to use the already loaded data, please move to the next section. Make sure that if you want to load the data again, load the pickled file or else the when the data is shuffled, some of the validation and test data might become part of the training data and the model will overfit on the test/validation data."]},{"cell_type":"code","metadata":{"id":"D0-yKODOO-0q"},"source":["image_dims = (120, 120)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1DFjIqet9M3"},"source":["X = [] # (no. of videos, no. of frames, frame height, frame width, no. of channels of frame)\n","y = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZGsD86XPCI6"},"source":["cheat_folder = os.path.relpath('<Relative path to Cheat folder>')\n","\n","# To load all files replace with len(files)\n","for root, dirs, files in os.walk(cheat_folder):\n","    for counter in tqdm(range(0, 4500)):\n","        filename = files[counter]\n","        # Open the video\n","\n","        cap = cv2.VideoCapture(cheat_folder + '/' + filename)\n","        frames = []\n","        while (cap.isOpened()):\n","            # Capture each frame\n","            ret, frame = cap.read()\n","\n","            # Check if a frame is capture\n","            if ret:\n","                # Resize the frame to image_dims x 3\n","                frame = cv2.resize(frame, image_dims, interpolation=cv2.INTER_CUBIC)\n","                # Store the frame in a list\n","                frames.append(np.array(frame))\n","            else:\n","                break\n","\n","        if len(frames) == 16:\n","            # Append video to x\n","            X.append(np.array(frames))\n","            # Add label 1 (cheating)\n","            y.append(1)\n","\n","        # Release video capture object\n","        cap.release()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyOGtk0DPEFL"},"source":["n_cheat_folder = os.path.relpath('<Relative path to N-Cheat folder>')\n","\n","# To download all files replace with len(files)\n","for root, dirs, files in os.walk(n_cheat_folder):\n","    for counter in tqdm(range(0, 4500)):\n","        filename = files[counter]\n","\n","        # Open the video\n","        cap = cv2.VideoCapture(n_cheat_folder + '/' + filename)\n","        frames = []\n","        while (cap.isOpened()):\n","            # Capture each frame\n","            ret, frame = cap.read()\n","\n","            # Check if a frame is capture\n","            if ret:\n","                # Resize the frame to image_dims x 3\n","                frame = cv2.resize(frame, image_dims, interpolation=cv2.INTER_CUBIC)\n","                # Store the frame in a list\n","                frames.append(np.array(frame))\n","            else:\n","                break\n","\n","        if len(frames) == 16:\n","            # Append video to x\n","            X.append(np.array(frames))\n","            # Add label 0 (not cheating)\n","            y.append(0)\n","\n","        # Release video capture object\n","        cap.release()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SNfNvHI4uGc7"},"source":["# shuffle dataset\n","temp = list(zip(X, y))\n","np.random.shuffle(temp)\n","X, y = zip(*temp)\n","del temp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SI00vMupuGc-"},"source":["# Pickle the data\n","with open('X', 'wb') as f:\n","    pickle.dump(X, f)\n","\n","with open('y', 'wb') as f:\n","    pickle.dump(y, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jtal-ajuGdB"},"source":["# Load pickled data\n","with open('X', 'rb') as f:\n","    X = pickle.load(f)\n","\n","with open('y', 'rb') as f:\n","    y = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqWcyGn3vR5_"},"source":["X = np.array(X, dtype=np.float32)\n","y = np.array(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P36eI_xXPOV8"},"source":["print('Input shape:', X.shape)\n","print('Output shape:', y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8QB8aTjdPTOz"},"source":["### Normalize Data"]},{"cell_type":"code","metadata":{"id":"yxKbmuQXPVik"},"source":["X = X / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"At_8u0-RPWrC"},"source":["x_train, x_val, y_train, y_val = X[:7500], X[7500:], y[:7500], y[7500:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VmMKSPMxuGdY"},"source":["# Delete extra variables, will save some space.\n","del X, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ygt2TbYtdza"},"source":["x_val, x_test, y_val, y_test = x_val[:750], x_val[750:], y_val[:750], y_val[750:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gm1uLew0PaM8"},"source":["# training set = 0.7, validation set = 0.15, test set = 0.15\n","print('Training set:', x_train.shape, y_train.shape, '\\n')\n","print('Validation set:', x_val.shape, y_val.shape, '\\n')\n","print('Test set:', x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Dl3b1nZPbRM"},"source":["# Training and test set sizes\n","train_set_size = x_train.shape[0]\n","val_set_size = x_val.shape[0]\n","test_set_size = x_test.shape[0]\n","classes = ['N-Cheat', 'Cheat']\n","num_of_classes = len(classes)\n","input_shape = (16, image_dims[0], image_dims[1], 3)\n","\n","BATCH_SIZE = 64\n","BUFFER_SIZE = len(x_train)\n","steps_per_epoch = train_set_size // BATCH_SIZE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4x89kKafuGdk"},"source":["lstm_units = 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzhlo2xauGdo"},"source":["# Shuffle dataset\n","train_dataset = list(zip(x_train, y_train))\n","\n","val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(val_set_size)\n","val_dataset = val_dataset.batch(val_set_size, drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZ4TTaicuGdr"},"source":["# Delete extra variables, will save some space.\n","del x_train, y_train\n","del x_val, y_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZNeF6uC-uGdw"},"source":["example_input_batch, example_target_batch = next(iter(val_dataset))\n","example_input_batch.shape, example_target_batch.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GPRuROgCPen1"},"source":["class CNN_LSTM(tf.keras.Model):\n","    def __init__(self, lstm_units, num_of_classes):\n","        super(CNN_LSTM, self).__init__()\n","        self.lstm_units = lstm_units\n","        # Initialize inceptionV3\n","        self.inceptionV3 = tf.keras.applications.InceptionV3(\n","            input_shape=(image_dims[0], image_dims[1], 3), include_top=False, weights='imagenet', pooling='avg')\n","        self.inceptionV3.trainable = False\n","        \n","        # Define LSTM\n","        self.lstm_1 = tf.keras.layers.LSTM(\n","            lstm_units, \n","            return_state=True,\n","            recurrent_initializer='glorot_uniform')\n","        \n","        self.fc = tf.keras.layers.Dense(num_of_classes)\n","\n","    def call(self, x, hidden_state, cell_state):\n","        # x (batch size, 16, image_dims[0], image_dims[1], 3)\n","        # hidden_state (batch size, lstm_units)\n","        # cell_state (batch size, lstm_units)\n","        for i in range(0, x.shape[1]):\n","            # x[:, i, :, :, :] # (batch size, image_dims[0], image_dims[1], 3)\n","            out = self.inceptionV3(x[:, i, :, :, :]) # (batch size, 1, 2048)\n","            out = tf.expand_dims(out, 1)\n","            _, hidden_state, cell_state = self.lstm_1(\n","                out, initial_state=[hidden_state, cell_state])\n","            \n","\n","        # Pass the last hidden state\n","        output = self.fc(hidden_state) # (batch size, num_of_classes)\n","\n","        return output\n","  \n","    def initialize_state(self, batch_size):\n","        return tf.zeros((batch_size, self.lstm_units)), tf.zeros((batch_size, self.lstm_units))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vs8Svs7qQq5U"},"source":["cnn_lstm = CNN_LSTM(lstm_units, num_of_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMEaKXH5R6ua"},"source":["# Load weights (Do not execute if weights are not saved)\n","cnn_lstm.load_weights('cnn_lstm weights 6/cnn_lstm_weights_6')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nPZQNfb6QsTy"},"source":["# Define loss function and optimizer\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","# Loss object\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","# Loss function\n","def loss_function(real, pred):\n","    # real (batch size,)\n","    # pred (batch size, num of classes)\n","    loss = loss_object(real, pred)\n","    # Take the mean loss of the batch\n","    loss = tf.reduce_mean(loss)\n","\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7DlBVdBuGeF"},"source":["lamda = 0.01\n","def reg_loss_function(real, pred, weights):\n","    # real (batch size,)\n","    # pred (batch size, num of classes)\n","    # weights: trainable_variables\n","    loss = loss_object(real, pred)\n","    \n","    # L2 Loss to penalize the weights\n","    reg_loss = 0\n","    for layer_weights in weights:\n","        reg_loss += tf.reduce_sum(tf.square(layer_weights))\n","        \n","    reg_loss = tf.math.divide((tf.reduce_sum(loss) + lamda * reg_loss), loss.shape[0])\n","    loss = tf.reduce_mean(loss)\n","    return loss, reg_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_UoIWowKRBaV"},"source":["def train_step(inp, targ):\n","    # Initialize hidden state and cell_state\n","    hidden_state, cell_state = cnn_lstm.initialize_state(inp.shape[0])\n","\n","    with tf.GradientTape() as tape:\n","        # Run the LSTM\n","        output = cnn_lstm(inp, hidden_state, cell_state)\n","\n","        # Calculate loss\n","        loss, reg_loss = reg_loss_function(targ, output, cnn_lstm.trainable_variables)\n","\n","    # Get the trainable variables\n","    trainable_variables = cnn_lstm.trainable_variables\n","\n","    # Take the gradient of trainable variables w.r.t loss\n","    gradients = tape.gradient(reg_loss, trainable_variables)\n","\n","    # Use the optimizer to back propagate and update weights\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","    return loss, tf.argmax(output, axis=1, output_type=tf.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfqttIc5R2yc"},"source":["train_loss_plot = []\n","val_loss_plot = []\n","train_acc_plot = []\n","val_acc_plot = []\n","\n","reg_train_loss_plot = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZLBdnozRvDc"},"source":["EPOCHS = 20\n","\n","for epoch in range(0, EPOCHS):\n","    start = time.time()\n","\n","    training_loss = 0\n","    val_loss = 0\n","\n","    training_accuracy = 0\n","    val_accuracy = 0\n","    \n","    # Shuffle dataset\n","    np.random.shuffle(train_dataset)\n","    \n","    for batch in range(0, len(train_dataset)):\n","        inp, labels = zip(*train_dataset[batch * BATCH_SIZE: batch * BATCH_SIZE + BATCH_SIZE])\n","        inp = np.array(inp)\n","        labels = np.array(labels)\n","        \n","        batch_loss, batch_predictions = train_step(inp, labels)\n","\n","        # Calculate training batch accuracy\n","        batch_accuracy = tf.reduce_sum(\n","            tf.cast(batch_predictions==labels, dtype=tf.float32)) / len(labels)\n","\n","        training_loss += batch_loss\n","        training_accuracy += batch_accuracy\n","\n","        if (batch + 1) % 10 == 0:\n","            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                            batch + 1,\n","                                            batch_loss.numpy()), \n","                 'Accuracy {:.4f}'.format(batch_accuracy.numpy()))\n","\n","    for (x_val, y_val) in val_dataset:\n","        # Find validation loss\n","        hidden_state, cell_state = cnn_lstm.initialize_state(x_val.shape[0])\n","        val_prediction_score = cnn_lstm(x_val, hidden_state, cell_state)\n","        val_loss = loss_function(y_val, val_prediction_score)\n","\n","        # Validation accuracy\n","        val_accuracy = tf.reduce_sum(\n","            tf.cast(\n","                tf.argmax(val_prediction_score, axis=1, output_type=tf.int32)==y_val, dtype=tf.float32)) / len(y_val)\n","\n","        \n","    train_loss_plot.append(training_loss / steps_per_epoch)\n","    val_loss_plot.append(val_loss.numpy())\n","\n","    train_acc_plot.append(training_accuracy / steps_per_epoch)\n","    val_acc_plot.append(val_accuracy)\n","   \n","    print('Training: Epoch {} Loss {:.4f}'.format(epoch + 1, training_loss / steps_per_epoch), \n","         'Accuracy {:.4f}'.format(training_accuracy / steps_per_epoch))\n","\n","    print('Validation: Epoch {} Loss {:.4f}'.format(epoch + 1, val_loss), \n","         'Accuracy {:.4f}'.format(val_accuracy))\n","\n","    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","    \n","    # Save weights after every epoch\n","    cnn_lstm.save_weights('cnn_lstm weights 6/cnn_lstm_weights_6', save_format='tf')\n","    # Save loss and accuracy after every epoch\n","    with open('cnn_lstm weights 6/train_loss_6', 'wb') as f:\n","        pickle.dump(train_loss_plot, f)\n","\n","    with open('cnn_lstm weights 6/val_loss_6', 'wb') as f:\n","        pickle.dump(val_loss_plot, f)\n","\n","    with open('cnn_lstm weights 6/train_acc_6', 'wb') as f:\n","        pickle.dump(train_acc_plot, f)\n","\n","    with open('cnn_lstm weights 6/val_acc_6', 'wb') as f:\n","        pickle.dump(val_acc_plot, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yy2PA1Fp3qwa"},"source":["# Load accuracy and loss\n","with open('cnn_lstm weights 6/train_loss_6', 'rb') as f:\n","    train_loss_plot = pickle.load(f)\n","    \n","with open('cnn_lstm weights 6/val_loss_6', 'rb') as f:\n","    val_loss_plot = pickle.load(f)\n","    \n","with open('cnn_lstm weights 6/train_acc_6', 'rb') as f:\n","    train_acc_plot =  pickle.load(f)\n","    \n","with open('cnn_lstm weights 6/val_acc_6', 'rb') as f:\n","    val_acc_plot = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bG358rbfuGeV"},"source":["plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss plot')\n","\n","plt.plot(train_loss_plot)\n","plt.plot(val_loss_plot)\n","\n","plt.legend(['Training', 'Validation'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_v46VU1TuGeb"},"source":["plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy plot')\n","\n","plt.plot(train_acc_plot)\n","plt.plot(val_acc_plot)\n","\n","plt.legend(['Training', 'Validation'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xU4JaHAJRtFg"},"source":["def evaluate(x):\n","    # Get intial hidden state and cell state\n","    hidden_state, cell_state = cnn_lstm.initialize_state(x.shape[0])\n","    # Predict\n","    predictions = cnn_lstm(x, hidden_state, cell_state)\n","    return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KlTLLmnNuGeg"},"source":["test_predictions = evaluate(x_test) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssBmfLrMuGei"},"source":["test_accuracy = tf.reduce_sum(\n","    tf.cast(tf.argmax(test_predictions, axis=1) == y_test, dtype=tf.float32)) / len(y_test)\n","    \n","test_loss = loss_function(y_test, test_predictions)\n","print('Test Accuracy:', test_accuracy)\n","print('Test Loss:', test_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X88sG-CTuGen"},"source":["cm = confusion_matrix(y_test, tf.argmax(test_predictions, axis=1, output_type=tf.int32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wwo_rOWXuGet"},"source":["def plot_confusion_matrix(cm,\n","                          target_names,\n","                          title='Confusion matrix',\n","                          cmap=None,\n","                          normalize=True):\n","    \"\"\"\n","    given a sklearn confusion matrix (cm), make a nice plot\n","\n","    Arguments\n","    ---------\n","    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n","\n","    target_names: given classification classes such as [0, 1, 2]\n","                  the class names, for example: ['high', 'medium', 'low']\n","\n","    title:        the text to display at the top of the matrix\n","\n","    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n","                  see http://matplotlib.org/examples/color/colormaps_reference.html\n","                  plt.get_cmap('jet') or plt.cm.Blues\n","\n","    normalize:    If False, plot the raw numbers\n","                  If True, plot the proportions\n","\n","    Usage\n","    -----\n","    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n","                                                              # sklearn.metrics.confusion_matrix\n","                          normalize    = True,                # show proportions\n","                          target_names = y_labels_vals,       # list of names of the classes\n","                          title        = best_estimator_name) # title of graph\n","\n","    Citiation\n","    ---------\n","    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n","\n","    \"\"\"\n","    import matplotlib.pyplot as plt\n","    import numpy as np\n","    import itertools\n","\n","    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n","    misclass = 1 - accuracy\n","\n","    if cmap is None:\n","        cmap = plt.get_cmap('Blues')\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","\n","    if target_names is not None:\n","        tick_marks = np.arange(len(target_names))\n","        plt.xticks(tick_marks, target_names, rotation=45)\n","        plt.yticks(tick_marks, target_names)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","\n","    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        if normalize:\n","            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","        else:\n","            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pemnTFf3uGew"},"source":["plot_confusion_matrix(cm, ['N-Cheat', 'Cheat'], normalize=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oh7yi5yuGey"},"source":[],"execution_count":null,"outputs":[]}]}